{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read\n",
      "data sorted\n",
      "data : 213446\n",
      "dataset created\n",
      "                     stime                    etime             sip  sport  \\\n",
      "0  2017-05-09 10:36:34.876  2017-05-09 11:05:37.894  192.168.80.102   5353   \n",
      "1  2017-05-09 10:38:11.077  2017-05-09 11:06:13.402  192.168.110.60  34056   \n",
      "2  2017-05-09 10:38:11.077  2017-05-09 11:06:13.402  192.168.110.60  34056   \n",
      "3  2017-05-09 10:38:11.077  2017-05-09 11:06:13.402  192.168.110.60  34056   \n",
      "4  2017-05-09 10:38:11.078  2017-05-09 11:06:13.402  192.168.110.60  34056   \n",
      "\n",
      "       sipint                 mac osname osversion fingerprint  \\\n",
      "0  3232256102  20:cf:30:8b:6f:17:                                \n",
      "1  3232263740  00:26:18:f0:62:08:                                \n",
      "2  3232263740  00:26:18:f0:62:08:                                \n",
      "3  3232263740  00:26:18:f0:62:08:                                \n",
      "4  3232263740  00:26:18:f0:62:08:                                \n",
      "\n",
      "               dip     ...     ruflags entropy rentropy tos rtos application  \\\n",
      "0      224.0.0.251     ...           0     108        0  00   00          53   \n",
      "1  171.122.234.240     ...           0     237        0  00   00           0   \n",
      "2  182.132.115.102     ...           0     237        0  00   00           0   \n",
      "3   182.40.209.121     ...           0     237        0  00   00           0   \n",
      "4    124.232.60.77     ...           0     237        0  00   00           0   \n",
      "\n",
      "  vlanint domain endreason        hash  \n",
      "0       0      0    active   876631092  \n",
      "1       0      0       eof  4007369167  \n",
      "2       0      0       eof  4079288409  \n",
      "3       0      0       eof  4085883462  \n",
      "4       0      0       eof   961105778  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "file5 = open('201811291521.txt', 'r')\n",
    "dataset = file5.read()\n",
    "print('data read')\n",
    "dataset = dataset.split('\\n')\n",
    "data = []\n",
    "for i in range(0, len(dataset)):\n",
    "    data.append(dataset[i].split('|'))\n",
    "data = sorted(data, key=itemgetter(0))\n",
    "data.pop(0)\n",
    "print('data sorted')\n",
    "\n",
    "headings = ['stime', 'etime', 'sip', 'sport', 'sipint', 'mac', 'osname', 'osversion', 'fingerprint', 'dip', 'dport', 'dipint', 'dstmac', 'rosname', 'rosversion', 'rfingerprint', 'protocol', 'pkts', 'bytes', 'rpkts', 'rbytes', 'dur', 'iflags', 'riflags', 'uflags', 'ruflags', 'entropy', 'rentropy', 'tos', 'rtos', 'application', 'vlanint', 'domain', 'endreason', 'hash']\n",
    "print('data : '+str(len(data)))\n",
    "# for item in data:\n",
    "#     item[0] = datetime.strptime(item[0], '%Y-%m-%d %H:%M:%S.%f')\n",
    "#     item[1] = datetime.strptime(item[1], '%Y-%m-%d %H:%M:%S.%f')\n",
    "#     print(item[0].strftime('%m/%d/%Y'))\n",
    "\n",
    "data = np.array(data)\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = headings\n",
    "print('dataset created')\n",
    "print(df.head())\n",
    "# print(df['protocol'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              sip  sport              dip dport protocol\n",
      "0  192.168.80.102   5353      224.0.0.251  5353       17\n",
      "1  192.168.110.60  34056  171.122.234.240  3395       17\n",
      "2  192.168.110.60  34056  182.132.115.102  3395       17\n",
      "3  192.168.110.60  34056   182.40.209.121  3395       17\n",
      "4  192.168.110.60  34056    124.232.60.77  3395       17\n",
      "213446\n"
     ]
    }
   ],
   "source": [
    "edited_df = df.drop(['stime','etime','sipint','mac','osname','osversion','fingerprint','dipint','dstmac','rosname','rosversion','rfingerprint','iflags','riflags','uflags','ruflags','entropy','rentropy','tos','rtos','application','vlanint','domain','hash','pkts','bytes','rpkts','rbytes','dur','endreason'],axis=1)\n",
    "print(edited_df.head())\n",
    "# Get one hot encoding of columns B\n",
    "one_hot = pd.get_dummies(edited_df['protocol'])\n",
    "# Drop column B as it is now encoded\n",
    "edited_df = edited_df.drop('protocol',axis = 1)\n",
    "headers = []\n",
    "for i in one_hot.columns:\n",
    "    headers.append('protocol_' + i)\n",
    "# Join the encoded df\n",
    "\n",
    "one_hot.columns = headers\n",
    "# edited_df = edited_df.join(one_hot)\n",
    "\n",
    "def correct_ip(s):\n",
    "    o = ''\n",
    "    if '.' in s:\n",
    "        for part in s.split('.'):\n",
    "            part = part.zfill(3)\n",
    "            o += part \n",
    "    else:\n",
    "        o = o.zfill(12)\n",
    "    o = o[:3] + '.' + o[3:]\n",
    "    o = o[:7] + '.' + o[7:]\n",
    "    o = o[:11] + '.' + o[11:]\n",
    "    return o\n",
    "\n",
    "def correct_port(s):\n",
    "    return(s.zfill(5))\n",
    "        \n",
    "\n",
    "\n",
    "sip_headers = []\n",
    "dip_headers = []\n",
    "\n",
    "for i in range(4):\n",
    "    sip_headers.append('sip_'+str(i))\n",
    "    dip_headers.append('dip_'+str(i))\n",
    "\n",
    "sip = []\n",
    "for ip in edited_df['sip']:\n",
    "    sip.append(map(int,correct_ip(ip).split('.')))\n",
    "\n",
    "dip = []\n",
    "for ip in edited_df['dip']:\n",
    "    dip.append(map(int,correct_ip(ip).split('.')))\n",
    "        \n",
    "sport = []\n",
    "for port in edited_df['sport']:\n",
    "    sport.append(int(port))\n",
    "    \n",
    "dport = []\n",
    "for port in edited_df['dport']:\n",
    "    dport.append(int(port))\n",
    "# print(len(sip[0]))\n",
    "# print(len(dip[0]))\n",
    "# print(len(dport[0]))\n",
    "# print(len(sport[0]))\n",
    "8\n",
    "sip_df = pd.DataFrame(sip,columns=sip_headers)\n",
    "dip_df = pd.DataFrame(dip,columns=dip_headers)\n",
    "sport_df = pd.DataFrame(sport,columns=['sport'])\n",
    "dport_df = pd.DataFrame(sport,columns=['dport'])\n",
    "\n",
    "\n",
    "result = pd.concat([sip_df, dip_df, sport_df, dport_df, one_hot], axis=1, sort=False)\n",
    "result.head()\n",
    "result.describe()\n",
    "print(len(result.values))\n",
    "# sport_df\n",
    "# edited_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [6] vs. [32]\n\t [[{{node training_4/Adam/gradients/loss_5/dense_30_loss/mul_1_grad/BroadcastGradientArgs}} = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@training_4/Adam/gradients/loss_5/dense_30_loss/mul_1_grad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_4/Adam/gradients/loss_5/dense_30_loss/mul_1_grad/Shape, training_4/Adam/gradients/loss_5/dense_30_loss/mul_1_grad/Shape_1)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3919f7a03b68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvae_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [6] vs. [32]\n\t [[{{node training_4/Adam/gradients/loss_5/dense_30_loss/mul_1_grad/BroadcastGradientArgs}} = BroadcastGradientArgs[T=DT_INT32, _class=[\"loc:@training_4/Adam/gradients/loss_5/dense_30_loss/mul_1_grad/Reshape\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](training_4/Adam/gradients/loss_5/dense_30_loss/mul_1_grad/Shape, training_4/Adam/gradients/loss_5/dense_30_loss/mul_1_grad/Shape_1)]]"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "m = 6\n",
    "n_z = 2\n",
    "n_epoch = 10\n",
    "\n",
    "input_size = 17\n",
    "hidden_size = 8\n",
    "\n",
    "\n",
    "# Q(z|X) -- encoder\n",
    "inputs = Input(shape=(input_size,))\n",
    "h_q = Dense(hidden_size, activation='relu')(inputs)\n",
    "mu = Dense(n_z, activation='linear')(h_q)\n",
    "log_sigma = Dense(n_z, activation='linear')(h_q)\n",
    "\n",
    "def sample_z(args):\n",
    "    mu, log_sigma = args\n",
    "    eps = K.random_normal(shape=(m, n_z), mean=0., stddev=1.)\n",
    "    return mu + K.exp(log_sigma / 2) * eps\n",
    "\n",
    "\n",
    "# Sample z ~ Q(z|X)\n",
    "z = Lambda(sample_z)([mu, log_sigma])\n",
    "# P(X|z) -- decoder\n",
    "decoder_hidden = Dense(hidden_size, activation='relu')\n",
    "decoder_out = Dense(input_size, activation='sigmoid')\n",
    "\n",
    "h_p = decoder_hidden(z)\n",
    "outputs = decoder_out(h_p)\n",
    "\n",
    "# Overall VAE model, for reconstruction and training\n",
    "vae = Model(inputs, outputs)\n",
    "\n",
    "# Encoder model, to encode input into latent variable\n",
    "# We use the mean as the output as it is the center point, the representative of the gaussian\n",
    "encoder = Model(inputs, mu)\n",
    "\n",
    "# Generator model, generate new data given latent variable z\n",
    "d_in = Input(shape=(n_z,))\n",
    "d_h = decoder_hidden(d_in)\n",
    "d_out = decoder_out(d_h)\n",
    "decoder = Model(d_in, d_out)\n",
    "\n",
    "def vae_loss(y_true, y_pred):\n",
    "    \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
    "    # E[log P(X|z)]\n",
    "    recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)\n",
    "    # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "    kl = 0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)\n",
    "\n",
    "    return recon + kl\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "vae.fit(result, result, batch_size=m, nb_epoch=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170756, 17) (42690, 17)\n",
      "Running Fold 1 / 10\n",
      "Train on 153681 samples, validate on 17075 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "# from sklearn.model_selection import KFold # import KFold\n",
    "from sklearn.model_selection import LeaveOneOut \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "x_train,x_test = train_test_split(result, test_size=0.2)\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "# kf = KFold(n_splits=2) # Define the split - into 2 folds \n",
    "# kf.get_n_splits(x_train) # returns the number of splitting iterations in the cross-validator\n",
    "# print(kf) \n",
    "# KFold(n_splits=2, random_state=None, shuffle=False)\n",
    "\n",
    "# loo = LeaveOneOut()\n",
    "# loo.get_n_splits(X)\n",
    "\n",
    "\n",
    "\n",
    "input_d = Input(shape=(17,))\n",
    "encoded = Dense(14, activation='relu')(input_d)\n",
    "encoded = Dense(10, activation='relu')(encoded)\n",
    "encoded = Dense(6, activation='relu')(encoded)\n",
    "encoded = Dense(3, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(6, activation='relu')(encoded)\n",
    "decoded = Dense(10, activation='relu')(decoded)\n",
    "decoded = Dense(14, activation='relu')(decoded)\n",
    "decoded = Dense(17, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input_d, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_folds = 10\n",
    "fold_len = int(len(x_train)/n_folds)\n",
    "cross_validation_scores = []\n",
    "for i in range(n_folds):\n",
    "    print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "    fold_train = [k for k in range(0,i*fold_len)] + [k for k in range((i+1)*fold_len,len(x_train))]\n",
    "    fold_test = [item for item in range(i*fold_len,(i+1)*fold_len)]\n",
    "    test = x_train.iloc[fold_test]\n",
    "    train = x_train.iloc[fold_train]\n",
    "    autoencoder.fit(train, train,\n",
    "            epochs=10,\n",
    "            batch_size=256,\n",
    "            shuffle=True,\n",
    "            validation_data = (test,test))\n",
    "    fold_validation_score = autoencoder.evaluate(test,test,verbose=0)\n",
    "    print(\"fold validation score = \", fold_validation_score)\n",
    "    cross_validation_scores.append(fold_validation_score[1] * 100)\n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cross_validation_scores), np.std(cross_validation_scores)))\n",
    "autoencoder.evaluate(x=x_test,y=x_test)    \n",
    "    \n",
    "# autoencoder.fit(x_train, x_train,\n",
    "#             epochs=100,\n",
    "#             batch_size=256,\n",
    "#             shuffle=True,\n",
    "#             validation_split = 0.2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sip_0            192\n",
      "sip_1            168\n",
      "sip_2            080\n",
      "sip_3            102\n",
      "dip_0            224\n",
      "dip_1            000\n",
      "dip_2            000\n",
      "dip_3            251\n",
      "sport           5353\n",
      "dport           5353\n",
      "protocol_1         0\n",
      "protocol_139       0\n",
      "protocol_17        1\n",
      "protocol_2         0\n",
      "protocol_41        0\n",
      "protocol_47        0\n",
      "protocol_6         0\n",
      "Name: 0, dtype: object\n",
      "['192' '168' '080' '102' '224' '000' '000' '251' '5353' '5353' 0 0 1 0 0 0\n",
      " 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_35 to have shape (17,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-946c287ab3e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlist_of_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_35 to have shape (17,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "list_of_loss = []#autoencoder.predict()\n",
    "print(result.iloc[0])\n",
    "for index, row in result.iterrows():\n",
    "    print(result.iloc[index].values)\n",
    "    list_of_loss.append(autoencoder.evaluate(row,row,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
