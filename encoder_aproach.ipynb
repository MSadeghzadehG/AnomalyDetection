{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read\n",
      "data sorted\n",
      "data : 213446\n",
      "dataset created\n",
      "                     stime                    etime             sip  sport  \\\n",
      "0  2017-05-09 10:36:34.876  2017-05-09 11:05:37.894  192.168.80.102   5353   \n",
      "1  2017-05-09 10:38:11.077  2017-05-09 11:06:13.402  192.168.110.60  34056   \n",
      "2  2017-05-09 10:38:11.077  2017-05-09 11:06:13.402  192.168.110.60  34056   \n",
      "3  2017-05-09 10:38:11.077  2017-05-09 11:06:13.402  192.168.110.60  34056   \n",
      "4  2017-05-09 10:38:11.078  2017-05-09 11:06:13.402  192.168.110.60  34056   \n",
      "\n",
      "       sipint                 mac osname osversion fingerprint  \\\n",
      "0  3232256102  20:cf:30:8b:6f:17:                                \n",
      "1  3232263740  00:26:18:f0:62:08:                                \n",
      "2  3232263740  00:26:18:f0:62:08:                                \n",
      "3  3232263740  00:26:18:f0:62:08:                                \n",
      "4  3232263740  00:26:18:f0:62:08:                                \n",
      "\n",
      "               dip     ...     ruflags entropy rentropy tos rtos application  \\\n",
      "0      224.0.0.251     ...           0     108        0  00   00          53   \n",
      "1  171.122.234.240     ...           0     237        0  00   00           0   \n",
      "2  182.132.115.102     ...           0     237        0  00   00           0   \n",
      "3   182.40.209.121     ...           0     237        0  00   00           0   \n",
      "4    124.232.60.77     ...           0     237        0  00   00           0   \n",
      "\n",
      "  vlanint domain endreason        hash  \n",
      "0       0      0    active   876631092  \n",
      "1       0      0       eof  4007369167  \n",
      "2       0      0       eof  4079288409  \n",
      "3       0      0       eof  4085883462  \n",
      "4       0      0       eof   961105778  \n",
      "\n",
      "[5 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "# import csv\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "# from datetime import datetime\n",
    "\n",
    "file5 = open('201811291521.txt', 'r')\n",
    "dataset = file5.read()\n",
    "print('data read')\n",
    "dataset = dataset.split('\\n')\n",
    "data = []\n",
    "for i in range(0, len(dataset)):\n",
    "    data.append(dataset[i].split('|'))\n",
    "data = sorted(data, key=itemgetter(0))\n",
    "data.pop(0)\n",
    "print('data sorted')\n",
    "\n",
    "headings = ['stime', 'etime', 'sip', 'sport', 'sipint', 'mac', 'osname', 'osversion', 'fingerprint', 'dip', 'dport', 'dipint', 'dstmac', 'rosname', 'rosversion', 'rfingerprint', 'protocol', 'pkts', 'bytes', 'rpkts', 'rbytes', 'dur', 'iflags', 'riflags', 'uflags', 'ruflags', 'entropy', 'rentropy', 'tos', 'rtos', 'application', 'vlanint', 'domain', 'endreason', 'hash']\n",
    "print('data : '+str(len(data)))\n",
    "# for item in data:\n",
    "#     item[0] = datetime.strptime(item[0], '%Y-%m-%d %H:%M:%S.%f')\n",
    "#     item[1] = datetime.strptime(item[1], '%Y-%m-%d %H:%M:%S.%f')\n",
    "#     print(item[0].strftime('%m/%d/%Y'))\n",
    "\n",
    "data = np.array(data)\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = headings\n",
    "print('dataset created')\n",
    "print(df.head())\n",
    "# print(df['protocol'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              sip  sport              dip dport protocol\n",
      "0  192.168.80.102   5353      224.0.0.251  5353       17\n",
      "1  192.168.110.60  34056  171.122.234.240  3395       17\n",
      "2  192.168.110.60  34056  182.132.115.102  3395       17\n",
      "3  192.168.110.60  34056   182.40.209.121  3395       17\n",
      "4  192.168.110.60  34056    124.232.60.77  3395       17\n",
      "213446\n"
     ]
    }
   ],
   "source": [
    "edited_df = df.drop(['stime','etime','sipint','mac','osname','osversion','fingerprint','dipint','dstmac','rosname','rosversion','rfingerprint','iflags','riflags','uflags','ruflags','entropy','rentropy','tos','rtos','application','vlanint','domain','hash','pkts','bytes','rpkts','rbytes','dur','endreason'],axis=1)\n",
    "print(edited_df.head())\n",
    "# Get one hot encoding of columns B\n",
    "one_hot = pd.get_dummies(edited_df['protocol'])\n",
    "# Drop column B as it is now encoded\n",
    "edited_df = edited_df.drop('protocol',axis = 1)\n",
    "headers = []\n",
    "for i in one_hot.columns:\n",
    "    headers.append('protocol_' + i)\n",
    "# Join the encoded df\n",
    "\n",
    "one_hot.columns = headers\n",
    "# edited_df = edited_df.join(one_hot)\n",
    "\n",
    "def correct_ip(s):\n",
    "    o = ''\n",
    "    if '.' in s:\n",
    "        for part in s.split('.'):\n",
    "            part = part.zfill(3)\n",
    "            o += part \n",
    "    else:\n",
    "        o = o.zfill(12)\n",
    "    o = o[:3] + '.' + o[3:]\n",
    "    o = o[:7] + '.' + o[7:]\n",
    "    o = o[:11] + '.' + o[11:]\n",
    "    return o\n",
    "\n",
    "def correct_port(s):\n",
    "    return(s.zfill(5))\n",
    "        \n",
    "\n",
    "\n",
    "sip_headers = []\n",
    "dip_headers = []\n",
    "\n",
    "for i in range(4):\n",
    "    sip_headers.append('sip_'+str(i))\n",
    "    dip_headers.append('dip_'+str(i))\n",
    "\n",
    "sip = []\n",
    "for ip in edited_df['sip']:\n",
    "    sip.append(map(str,correct_ip(ip).split('.')))\n",
    "#     sip.append(correct_ip(ip).split('.'))\n",
    "\n",
    "dip = []\n",
    "for ip in edited_df['dip']:\n",
    "    dip.append(map(str,correct_ip(ip).split('.')))\n",
    "#     dip.append(correct_ip(ip).split('.'))\n",
    "\n",
    "\n",
    "        \n",
    "sport = []\n",
    "for port in edited_df['sport']:\n",
    "    sport.append(int(port))\n",
    "    \n",
    "dport = []\n",
    "for port in edited_df['dport']:\n",
    "    dport.append(int(port))\n",
    "# print(len(sip[0]))\n",
    "# print(len(dip[0]))\n",
    "# print(len(dport[0]))\n",
    "# print(len(sport[0]))\n",
    "8\n",
    "sip_df = pd.DataFrame(sip,columns=sip_headers)\n",
    "dip_df = pd.DataFrame(dip,columns=dip_headers)\n",
    "sport_df = pd.DataFrame(sport,columns=['sport'])\n",
    "dport_df = pd.DataFrame(sport,columns=['dport'])\n",
    "\n",
    "\n",
    "result = pd.concat([sip_df, dip_df, sport_df, dport_df, one_hot], axis=1, sort=False)\n",
    "result.head()\n",
    "result.describe()\n",
    "print(len(result.values))\n",
    "# sport_df\n",
    "# edited_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Dense, Lambda\n",
    "# from keras.models import Model\n",
    "# from keras.objectives import binary_crossentropy\n",
    "# from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import keras.backend as K\n",
    "# import tensorflow as tf\n",
    "\n",
    "\n",
    "# m = 6\n",
    "# n_z = 2\n",
    "# n_epoch = 10\n",
    "\n",
    "# input_size = 17\n",
    "# hidden_size = 8\n",
    "\n",
    "\n",
    "# # Q(z|X) -- encoder\n",
    "# inputs = Input(shape=(input_size,))\n",
    "# h_q = Dense(hidden_size, activation='relu')(inputs)\n",
    "# mu = Dense(n_z, activation='linear')(h_q)\n",
    "# log_sigma = Dense(n_z, activation='linear')(h_q)\n",
    "\n",
    "# def sample_z(args):\n",
    "#     mu, log_sigma = args\n",
    "#     eps = K.random_normal(shape=(m, n_z), mean=0., stddev=1.)\n",
    "#     return mu + K.exp(log_sigma / 2) * eps\n",
    "\n",
    "\n",
    "# # Sample z ~ Q(z|X)\n",
    "# z = Lambda(sample_z)([mu, log_sigma])\n",
    "# # P(X|z) -- decoder\n",
    "# decoder_hidden = Dense(hidden_size, activation='relu')\n",
    "# decoder_out = Dense(input_size, activation='sigmoid')\n",
    "\n",
    "# h_p = decoder_hidden(z)\n",
    "# outputs = decoder_out(h_p)\n",
    "\n",
    "# # Overall VAE model, for reconstruction and training\n",
    "# vae = Model(inputs, outputs)\n",
    "\n",
    "# # Encoder model, to encode input into latent variable\n",
    "# # We use the mean as the output as it is the center point, the representative of the gaussian\n",
    "# encoder = Model(inputs, mu)\n",
    "\n",
    "# # Generator model, generate new data given latent variable z\n",
    "# d_in = Input(shape=(n_z,))\n",
    "# d_h = decoder_hidden(d_in)\n",
    "# d_out = decoder_out(d_h)\n",
    "# decoder = Model(d_in, d_out)\n",
    "\n",
    "# def vae_loss(y_true, y_pred):\n",
    "#     \"\"\" Calculate loss = reconstruction loss + KL loss for each data in minibatch \"\"\"\n",
    "#     # E[log P(X|z)]\n",
    "#     recon = K.sum(K.binary_crossentropy(y_pred, y_true), axis=1)\n",
    "#     # D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "#     kl = 0.5 * K.sum(K.exp(log_sigma) + K.square(mu) - 1. - log_sigma, axis=1)\n",
    "\n",
    "#     return recon + kl\n",
    "\n",
    "# vae.compile(optimizer='adam', loss=vae_loss)\n",
    "# vae.fit(result, result, batch_size=m, nb_epoch=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170756, 17) (42690, 17)\n",
      "Train on 136604 samples, validate on 34152 samples\n",
      "Epoch 1/100\n",
      "136604/136604 [==============================] - 6s 48us/step - loss: -46970.4359 - acc: 0.3514 - val_loss: -89009.9499 - val_acc: 0.3683\n",
      "Epoch 2/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7426 - acc: 0.3659 - val_loss: -89009.9754 - val_acc: 0.3682\n",
      "Epoch 3/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88755.7548 - acc: 0.3654 - val_loss: -89009.9749 - val_acc: 0.3695\n",
      "Epoch 4/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7619 - acc: 0.3676 - val_loss: -89009.9791 - val_acc: 0.3716\n",
      "Epoch 5/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88755.7642 - acc: 0.3682 - val_loss: -89009.9783 - val_acc: 0.3741\n",
      "Epoch 6/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88755.7648 - acc: 0.3681 - val_loss: -89009.9809 - val_acc: 0.3692\n",
      "Epoch 7/100\n",
      "136604/136604 [==============================] - 4s 31us/step - loss: -88755.7672 - acc: 0.3704 - val_loss: -89009.9823 - val_acc: 0.3720\n",
      "Epoch 8/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88755.7674 - acc: 0.3703 - val_loss: -89009.9824 - val_acc: 0.3732\n",
      "Epoch 9/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88755.7689 - acc: 0.3719 - val_loss: -89009.9833 - val_acc: 0.3731\n",
      "Epoch 10/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7692 - acc: 0.3721 - val_loss: -89009.9839 - val_acc: 0.3733\n",
      "Epoch 11/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.7692 - acc: 0.3723 - val_loss: -89009.9805 - val_acc: 0.3729\n",
      "Epoch 12/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7700 - acc: 0.3735 - val_loss: -89009.9833 - val_acc: 0.3737\n",
      "Epoch 13/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7692 - acc: 0.3732 - val_loss: -89009.9838 - val_acc: 0.3737\n",
      "Epoch 14/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7710 - acc: 0.3742 - val_loss: -89009.9854 - val_acc: 0.3738\n",
      "Epoch 15/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7722 - acc: 0.3745 - val_loss: -89009.9858 - val_acc: 0.3743\n",
      "Epoch 16/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7728 - acc: 0.3749 - val_loss: -89009.9849 - val_acc: 0.3756\n",
      "Epoch 17/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7737 - acc: 0.3752 - val_loss: -89009.9874 - val_acc: 0.3753\n",
      "Epoch 18/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7746 - acc: 0.3755 - val_loss: -89009.9889 - val_acc: 0.3757\n",
      "Epoch 19/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7766 - acc: 0.3759 - val_loss: -89009.9918 - val_acc: 0.3754\n",
      "Epoch 20/100\n",
      "136604/136604 [==============================] - 4s 31us/step - loss: -88755.7411 - acc: 0.3731 - val_loss: -89009.9766 - val_acc: 0.3656\n",
      "Epoch 21/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7653 - acc: 0.3710 - val_loss: -89009.9820 - val_acc: 0.3750\n",
      "Epoch 22/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7670 - acc: 0.3730 - val_loss: -89009.9822 - val_acc: 0.3758\n",
      "Epoch 23/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7674 - acc: 0.3730 - val_loss: -89009.9823 - val_acc: 0.3744\n",
      "Epoch 24/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7691 - acc: 0.3737 - val_loss: -89009.9811 - val_acc: 0.3762\n",
      "Epoch 25/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7699 - acc: 0.3743 - val_loss: -89009.9840 - val_acc: 0.3740\n",
      "Epoch 26/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7711 - acc: 0.3749 - val_loss: -89009.9842 - val_acc: 0.3759\n",
      "Epoch 27/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7745 - acc: 0.3750 - val_loss: -89009.9990 - val_acc: 0.3761\n",
      "Epoch 28/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.7476 - acc: 0.3719 - val_loss: -89009.9899 - val_acc: 0.3761\n",
      "Epoch 29/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7764 - acc: 0.3762 - val_loss: -89009.9938 - val_acc: 0.3769\n",
      "Epoch 30/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7840 - acc: 0.3777 - val_loss: -89010.0017 - val_acc: 0.3812\n",
      "Epoch 31/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88755.7953 - acc: 0.3852 - val_loss: -89010.0139 - val_acc: 0.3918\n",
      "Epoch 32/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7307 - acc: 0.3815 - val_loss: -89009.9819 - val_acc: 0.3754\n",
      "Epoch 33/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88755.7671 - acc: 0.3726 - val_loss: -89009.9846 - val_acc: 0.3763\n",
      "Epoch 34/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88755.7699 - acc: 0.3755 - val_loss: -89009.9835 - val_acc: 0.3771\n",
      "Epoch 35/100\n",
      "136604/136604 [==============================] - 4s 31us/step - loss: -88755.7702 - acc: 0.3755 - val_loss: -89009.9859 - val_acc: 0.3761\n",
      "Epoch 36/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88755.7722 - acc: 0.3759 - val_loss: -89009.9857 - val_acc: 0.3753\n",
      "Epoch 37/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7745 - acc: 0.3760 - val_loss: -89009.9887 - val_acc: 0.3759\n",
      "Epoch 38/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.5562 - acc: 0.3745 - val_loss: -89009.1511 - val_acc: 0.3547\n",
      "Epoch 39/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6777 - acc: 0.3391 - val_loss: -89008.8937 - val_acc: 0.3394\n",
      "Epoch 40/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6758 - acc: 0.3390 - val_loss: -89008.8942 - val_acc: 0.3394\n",
      "Epoch 41/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6751 - acc: 0.3391 - val_loss: -89008.8942 - val_acc: 0.3394\n",
      "Epoch 42/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6756 - acc: 0.3391 - val_loss: -89008.8942 - val_acc: 0.3392\n",
      "Epoch 43/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6757 - acc: 0.3391 - val_loss: -89008.8944 - val_acc: 0.3396\n",
      "Epoch 44/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6756 - acc: 0.3391 - val_loss: -89008.8943 - val_acc: 0.3395\n",
      "Epoch 45/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6755 - acc: 0.3391 - val_loss: -89008.8941 - val_acc: 0.3392\n",
      "Epoch 46/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88754.6755 - acc: 0.3391 - val_loss: -89008.8945 - val_acc: 0.3396\n",
      "Epoch 47/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6758 - acc: 0.3391 - val_loss: -89008.8945 - val_acc: 0.3395\n",
      "Epoch 48/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88754.6756 - acc: 0.3392 - val_loss: -89008.8948 - val_acc: 0.3396\n",
      "Epoch 49/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88754.6763 - acc: 0.3392 - val_loss: -89008.8947 - val_acc: 0.3397\n",
      "Epoch 50/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88754.6758 - acc: 0.3392 - val_loss: -89008.8947 - val_acc: 0.3397\n",
      "Epoch 51/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88754.6764 - acc: 0.3393 - val_loss: -89008.8947 - val_acc: 0.3398\n",
      "Epoch 52/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.7720 - acc: 0.3450 - val_loss: -89009.1137 - val_acc: 0.3531\n",
      "Epoch 53/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.8837 - acc: 0.3512 - val_loss: -89008.8713 - val_acc: 0.3387\n",
      "Epoch 54/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6646 - acc: 0.3384 - val_loss: -89008.8821 - val_acc: 0.3386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6634 - acc: 0.3385 - val_loss: -89008.8836 - val_acc: 0.3389\n",
      "Epoch 56/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6691 - acc: 0.3384 - val_loss: -89008.8909 - val_acc: 0.3389\n",
      "Epoch 57/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88754.7909 - acc: 0.3456 - val_loss: -89009.1370 - val_acc: 0.3539\n",
      "Epoch 58/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88754.8802 - acc: 0.3509 - val_loss: -89009.1141 - val_acc: 0.3538\n",
      "Epoch 59/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88754.8244 - acc: 0.3480 - val_loss: -89008.8874 - val_acc: 0.3386\n",
      "Epoch 60/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88754.8656 - acc: 0.3504 - val_loss: -89009.1274 - val_acc: 0.3538\n",
      "Epoch 61/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.9212 - acc: 0.3533 - val_loss: -89008.8867 - val_acc: 0.3388\n",
      "Epoch 62/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88754.6697 - acc: 0.3384 - val_loss: -89008.8913 - val_acc: 0.3387\n",
      "Epoch 63/100\n",
      "136604/136604 [==============================] - 4s 31us/step - loss: -88754.8548 - acc: 0.3495 - val_loss: -89009.1194 - val_acc: 0.3530\n",
      "Epoch 64/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88754.9060 - acc: 0.3530 - val_loss: -89009.1233 - val_acc: 0.3531\n",
      "Epoch 65/100\n",
      "136604/136604 [==============================] - 4s 30us/step - loss: -88753.8779 - acc: 0.3577 - val_loss: -89009.3664 - val_acc: 0.3683\n",
      "Epoch 66/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.0373 - acc: 0.3596 - val_loss: -89009.1506 - val_acc: 0.3530\n",
      "Epoch 67/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88754.9357 - acc: 0.3534 - val_loss: -89009.1434 - val_acc: 0.3538\n",
      "Epoch 68/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.1354 - acc: 0.3536 - val_loss: -89009.4323 - val_acc: 0.3565\n",
      "Epoch 69/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.5697 - acc: 0.3653 - val_loss: -89009.9780 - val_acc: 0.3745\n",
      "Epoch 70/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7663 - acc: 0.3726 - val_loss: -89009.9811 - val_acc: 0.3733\n",
      "Epoch 71/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7686 - acc: 0.3732 - val_loss: -89009.9809 - val_acc: 0.3744\n",
      "Epoch 72/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7684 - acc: 0.3731 - val_loss: -89009.9808 - val_acc: 0.3745\n",
      "Epoch 73/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7684 - acc: 0.3730 - val_loss: -89009.9819 - val_acc: 0.3741\n",
      "Epoch 74/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7684 - acc: 0.3726 - val_loss: -89009.9801 - val_acc: 0.3708\n",
      "Epoch 75/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7677 - acc: 0.3725 - val_loss: -89009.9814 - val_acc: 0.3744\n",
      "Epoch 76/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7684 - acc: 0.3729 - val_loss: -89009.9822 - val_acc: 0.3727\n",
      "Epoch 77/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.7687 - acc: 0.3724 - val_loss: -89009.9827 - val_acc: 0.3738\n",
      "Epoch 78/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.7693 - acc: 0.3733 - val_loss: -89009.9823 - val_acc: 0.3732\n",
      "Epoch 79/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.4863 - acc: 0.3711 - val_loss: -89009.3919 - val_acc: 0.3684\n",
      "Epoch 80/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.1804 - acc: 0.3685 - val_loss: -89009.4124 - val_acc: 0.3681\n",
      "Epoch 81/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.7969 - acc: 0.3454 - val_loss: -89008.8895 - val_acc: 0.3386\n",
      "Epoch 82/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.6755 - acc: 0.3384 - val_loss: -89008.8915 - val_acc: 0.3386\n",
      "Epoch 83/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.0974 - acc: 0.3630 - val_loss: -89009.1492 - val_acc: 0.3534\n",
      "Epoch 84/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88754.9309 - acc: 0.3522 - val_loss: -89009.4325 - val_acc: 0.3567\n",
      "Epoch 85/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.6724 - acc: 0.3691 - val_loss: -89009.9800 - val_acc: 0.3747\n",
      "Epoch 86/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.7677 - acc: 0.3746 - val_loss: -89009.9808 - val_acc: 0.3752\n",
      "Epoch 87/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.7679 - acc: 0.3738 - val_loss: -89009.9819 - val_acc: 0.3731\n",
      "Epoch 88/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.7697 - acc: 0.3739 - val_loss: -89009.9836 - val_acc: 0.3738\n",
      "Epoch 89/100\n",
      "136604/136604 [==============================] - 4s 27us/step - loss: -88755.7699 - acc: 0.3737 - val_loss: -89009.9831 - val_acc: 0.3731\n",
      "Epoch 90/100\n",
      "136604/136604 [==============================] - 4s 28us/step - loss: -88755.7702 - acc: 0.3736 - val_loss: -89009.9839 - val_acc: 0.3732\n",
      "Epoch 91/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7705 - acc: 0.3736 - val_loss: -89009.9820 - val_acc: 0.3729\n",
      "Epoch 92/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88748.1755 - acc: 0.3707 - val_loss: -89009.6939 - val_acc: 0.3717\n",
      "Epoch 93/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.2648 - acc: 0.3554 - val_loss: -89009.4382 - val_acc: 0.3559\n",
      "Epoch 94/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.3588 - acc: 0.3607 - val_loss: -89009.6877 - val_acc: 0.3713\n",
      "Epoch 95/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.4815 - acc: 0.3685 - val_loss: -89009.7037 - val_acc: 0.3708\n",
      "Epoch 96/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.5579 - acc: 0.3660 - val_loss: -89009.9701 - val_acc: 0.3733\n",
      "Epoch 97/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7446 - acc: 0.3657 - val_loss: -89009.9629 - val_acc: 0.3681\n",
      "Epoch 98/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7584 - acc: 0.3679 - val_loss: -89009.9799 - val_acc: 0.3740\n",
      "Epoch 99/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7529 - acc: 0.3671 - val_loss: -89009.9808 - val_acc: 0.3752\n",
      "Epoch 100/100\n",
      "136604/136604 [==============================] - 4s 29us/step - loss: -88755.7458 - acc: 0.3657 - val_loss: -89009.9817 - val_acc: 0.3747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1af8309860>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     print(row)\n",
    "x_train,x_test = train_test_split(result, test_size=0.2)\n",
    "print(x_train.shape, x_test.shape)\n",
    "\n",
    "# kf = KFold(n_splits=2) # Define the split - into 2 folds \n",
    "# kf.get_n_splits(x_train) # returns the number of splitting iterations in the cross-validator\n",
    "# print(kf) \n",
    "# KFold(n_splits=2, random_state=None, shuffle=False)\n",
    "\n",
    "# loo = LeaveOneOut()\n",
    "# loo.get_n_splits(X)\n",
    "\n",
    "\n",
    "\n",
    "input_d = Input(shape=(17,))\n",
    "encoded = Dense(14, activation='relu')(input_d)\n",
    "encoded = Dense(10, activation='relu')(encoded)\n",
    "encoded = Dense(6, activation='relu')(encoded)\n",
    "encoded = Dense(3, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(6, activation='relu')(encoded)\n",
    "decoded = Dense(10, activation='relu')(decoded)\n",
    "decoded = Dense(14, activation='relu')(decoded)\n",
    "decoded = Dense(17, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input_d, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# # history = None\n",
    "# n_folds = 10\n",
    "# fold_len = int(len(x_train)/n_folds)\n",
    "# cross_validation_scores = []\n",
    "# for i in range(n_folds):\n",
    "#     print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "#     fold_train = [k for k in range(0,i*fold_len)] + [k for k in range((i+1)*fold_len,len(x_train))]\n",
    "#     fold_test = [item for item in range(i*fold_len,(i+1)*fold_len)]\n",
    "#     test = x_train.iloc[fold_test]\n",
    "#     train = x_train.iloc[fold_train]\n",
    "#     history = autoencoder.fit(train, train,\n",
    "#             epochs=10,\n",
    "#             batch_size=256,\n",
    "#             shuffle=True,\n",
    "#             validation_data = (test,test))\n",
    "#     fold_validation_score = autoencoder.evaluate(test,test,verbose=0)\n",
    "#     print(\"fold validation score = \", fold_validation_score)\n",
    "#     cross_validation_scores.append(fold_validation_score[1] * 100)\n",
    "#     # summarize history for accuracy\n",
    "#     plt.plot(history.history['acc'])\n",
    "#     plt.plot(history.history['val_acc'])\n",
    "#     plt.title('model accuracy')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()\n",
    "#     # summarize history for loss\n",
    "#     plt.plot(history.history['loss'])\n",
    "#     plt.plot(history.history['val_loss'])\n",
    "#     plt.title('model loss')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()\n",
    "\n",
    "# print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cross_validation_scores), np.std(cross_validation_scores)))\n",
    "# autoencoder.evaluate(x=x_test,y=x_test)    \n",
    "    \n",
    "autoencoder.fit(x_train, x_train,\n",
    "            epochs=100,\n",
    "            batch_size=256,\n",
    "            shuffle=True,\n",
    "            validation_split = 0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_loss = []#autoencoder.predict()\n",
    "print(result.iloc[0])\n",
    "for index, row in result.iterrows():\n",
    "    print(result.iloc[index].values)\n",
    "    list_of_loss.append(autoencoder.evaluate(row,row,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
